Encoding Categorical data:

-> no ordering: One-hot encoder (e.g. country name)
-> ordering exists: Label encoder (e.g. t-shirt size)


Assumptions of Linear Regression:

-> Linearity: Linear relationship between Y and each X
-> Homoscedasticity: Equal Variance
-> Multivariate Normality: Normality of error distribution
-> Independence: No autocorrelation (a point in a graph should not depend upon the value of previous point)
-> Lack of Multicollinearity: Predictors are not correlated with each other

Safety Check:

-> The Outlier Check: Remove them before training or include them (depends on usecase)

Methods of building models:

-> All-in: Take all the features into consideration
-> Stepwise Regression
	-> Backward Elimination
		- Step 1: Select a significance level to stay in the model (e.g. SL = 0.05)
		- Step 2: Fit the full model with all possible predictors
		- Step 3: Consider the predictor with the highest P-value. If P > SL, go to STEP 4, otherwise FINISH
		- Step 4: Remove the predictor
		- Step 5: Fit the model without this variable (which is removed in Step 4). Go to STEP 3
	-> Forward Selection
		- Step 1: Select a significance level to enter the model (e.g. SL = 0.05)
		- Step 2: Fit all simple regression models y~x(n). Select the one with the lowest P-value
		- Step 3: Keep this variable and fit all possible models with one extra predictor added to the one(s) you already have
		- Step 4: Consider the predictor with lowest P-value. If P < SL, go to STEP 3, otherwise FINISH
	-> Bidirectional Elimination
		- Step 1: Select a significance level to enter and to stay in the model (e.g. SLENTER = 0.05, SLSTAY = 0.05)
		- Step 2: Perform the next step of Forward Selection (new variables must have: P < SLENTER to enter)
		- Step 3: Perform all steps of Backward Elimination (old variables must have P < SLSTAY to stay). Go to STEP 2
		- Step 4: No new variables can enter and no old variables can exit. FINISH
-> Score Comparison
	- Step 1: Select a criterionof goodness of fit (e.g. Akaike criterion)
	- Step 2: Construct all possible regression models (2^n - 1) total combinations
	- Step 3: Select the one with the best criterion. FINISH

Random Forest:

-> Step 1: Pick at random K data points from the Training set
-> Step 2: Build the Decision Tree associated to these K data points.
-> Step 3: Choose the number NTree of trees you want to build and repeat STEPS 1 & 2.
-> Step 4: For a new data point, make each one of your Ntree trees predictthe value of Y to for the data point in question, and assign the new data point the average across all of the predicted Y values. FINISH

Evaluating Regression Models Performance:

-> Residual Sum of Squares (SS(res)) = SUM((y(actual) - y(predicted))^2)
-> Total Sum of Squares (SS(tot)) = SUM((y(actual) - y(average-actual))^2)
-> R Squared (R^2) = 1 - SS(res)/SS(tot), tells the goodness of fit
-> greater the R^2 -> better the model
-> Adjusted R^2 = 1 - (1-R^2)*((n-1)/(n-k-1))
	-> k - number of independent variables
	-> n - sample size
